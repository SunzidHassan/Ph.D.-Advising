### Main Research Goal
- Fuse vision and olfaction to find the odor source.
- Vision: use a camera to detect objects from the environment and use a LLM to identify which object is a possible odor source.
  - Problem 1: universal object detector (to extract objects from the observed images)
  - Problem 2: determine extracted object positions (less important) -> we will have a better search trajectory
- Olfaction: guide the robot to approach the odor source location when no valid visual targets are available.
  - Solve it with moth-inspired method

### Task 1: Write a research plan (in few sentences) to fulfill the main research goal.
- How to solve the problem 1?
  - Approach 1 - image classification using VLM (e.g., CLIP)
    - Provide an image and a list of objects to CLIP VLM. It assigns greater probability to the object it considers to be represented in the image.
    - **Updated in April 22nd: Try These Methods with Sample Images Captured from the Previous Test** 
  - Approach 2 - semantic segmentation using VLM
    - Provide an image and a list of objects to CLIPSeg VLM. If we provide a list of n objects to CLIPSeg, it generates n copies of the image - one for each object in the lsit. Each copy includes a segmentation mask of the objects, i.e., each copy masks the portion of the image with that object.
  - Approach 3 - image captioning or visual question answering using VLM
    - Provide an image to visual question answering (VQA) model (e.g., BLIP or BLIP-2). The model can generate a caption for the image, or answer question about the image.

- Do you have better framework in the vision branch?

- How to solve the problem 2?
  - Approach 1 - object grounding using depth camera: depth camera publishes point cloud indicating distances of objects in the visual field.
  - Approach 2 - using 2-D LiDAR and RGB camera: https://www.mdpi.com/2076-3417/10/17/5782

### Task 2: Create a Simulation Environment to Evaluate Robotic OSL
- Goal 1: a rectangular environment with one odor source and multiple other objects
  - Problem 1: find a library for other 3D realistic objects
    - Approach 1: using Gazebo simulation environment. Testing if image classification method correctly classifies both real and simulated objects.
  - Problem 2: robotic agent itself (sensors, controls, sensor feedbacks)
    - Approach 1: using Gazebo simulation environment.
      - Sensors: Available sensors: camra, LDS, depth, airflow and wind direction. Couldn't find existing chemical sensor plugin.
      - Controls: similar to real-world control.
      - Sensor feedback: subscription similar to real-world robot.

- Goal 2: plume simulation in the simulation environment
  - Problem 1: how to simulate plume movements in this simulation environment? (Gaden Maybe? Use Real-world plume data maybe?)
   - Approach 1: Gaden tutorial: https://colab.research.google.com/drive/1Xj7rrsmeDa_dS3Ru_UIhhzlaifGH6GS4

