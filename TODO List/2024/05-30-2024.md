
### Implement the LLM to the Vision-Olfaction Navigation
#### The vision branch
- **Objective**: Use vision model to process input images at a cell
  - The goal is to let the vision model to indicate which object is the possible odor source object
- **Steps**
  - The purpose of the vision model is to 1: detect objects within the view; 2: determine which one is the possible odor source
    - Step 1: Detect objects within the view
      - Possible method 1: YOLO to extract objects from the image
      - Possible method 2: CLIPSeg to extract objects from the image
      - Possible method 3: Mask R-CNN to extract objects from the image
      - The output of step 1 is a list of object names: e.g., ['humidifier', 'coke can', 'fans', ....]
    - Step 2: Determine which one is the possible odor source
      - Possible method 1: Use BLIP (GPT-3, etc.) to determine which object is the possible odor source.
      - Input: list of object names
      - Output: the name of the object that is most likely the odor source.
  - How do you train object detection model?
    - You label objects within the training images
    - You train a YOLO model (or other object detection models) to detect these objects from the image
  - How do you train language model?
    - You provide some examples to the LLM, e.g., input: ['humidifier', 'coke can', ...]; Reason: "since the humidifier can send odors into the environment, humidifier is a possible odor source within this list."; Output: humidifier.       
#### The Olfaction branch
- [ ] Import the wind/chemical information into LLM
#### The Decision-making Model
- [ ] This is a LLM to process vision and olfaction detection results
