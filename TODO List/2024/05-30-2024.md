
### Implement the LLM to the Vision-Olfaction Navigation
#### The vision branch
- **Objective**: Use vision model to process input images at a cell
  - The goal is to let the vision model to indicate which object is the possible odor source object
- **Steps**
  - The purpose of the vision model is to 1: detect objects within the view; 2: determine which one is the possible odor source
    - [x] Step 1: Detect objects within the view
      - Possible method 1: YOLO to extract objects from the image
      - Possible method 2: CLIPSeg to extract objects from the image
      - Possible method 3: Mask R-CNN to extract objects from the image
      - The output of step 1 is a list of object names: e.g., ['humidifier', 'coke can', 'fans', ....]
    - Step 2: Determine which one is the possible odor source
      - Possible method 1: Use BLIP (GPT-3, etc.) to determine which object is the possible odor source.
      - Input: list of object names
      - Output: the name of the object that is most likely the odor source.
      - **Update in 06-10-2024**:
        - add one more column in the sensor md table to show the possible odor source object
        - OR replace the last column with the possible odor source object

  - How do you train language model?
    - You provide some examples to the LLM, e.g., input: ['humidifier', 'coke can', ...]; Reason: "since the humidifier can send odors into the environment, humidifier is a possible odor source within this list."; Output: humidifier.
    - **Update in 06-10-2024**
      - Discuss with khan and add the memory module into the LLM decision cycle.
      - Core idea is to provide some successful examples to the LLM when it makes the action decision
      - How to define successful action? many possibilities: not hitting the obstacle/go to high concentration location/go against wind       
#### The Olfaction branch
- [x] Import the wind/chemical information into LLM
#### The Decision-making Model
- [x] This is a LLM to process vision and olfaction detection results
- **Update in 06-10-2024**:
  - We need a loop decision-making framework
    - [x] The robot starts at an initial location
    - [x] The LLM makes a decision / let DQN makes a decision (Input: olfactory sensor data; Output: robot action)
    - [x] The robot moves to the new location
    - [x] Repeat until the robot finds the correct odor source
  - Train a DQN agent to make decisions (so far we can only do olfaction-based navigation)


#### Update in 06-17-2024
- [ ] YOLO class cannot have "humdifier box"
  - Remove humidifier box from the detection list
- [ ] Olfaction needs to be as important as vision
  - [ ] Edit the decision-making criteria
  - [ ] We want to see a case where the "facing odor source" is all 0, how the LLM reacts (you can mannually create this situation)
- [ ] Make the search area larger (the grid size can change) 
